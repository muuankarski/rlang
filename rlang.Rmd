---
title:  'Open research methods in computational social sciences and humanities: introducing R'
author: Markus Kainu
date: "`r Sys.time()`"
affiliation: "National Social insurance institution of Finland (Kela)"
tags: [r-language, open science]
output: html_document
# output: 
  # tint::tintPdf: 
  #   citation_package: natbib
  #   latex_engine: pdflatex
bibliography: bibtex.bib
lang: english
...


# Introduction - Open Research Methods


The debate on open science in the context of Social Sciences and Humanities (SSH) has been predominantly focusing on open access to research publication and opening up the various types of digital research data (open research data). Discussion on the *openness of research methods* has spring to life only recently.

I can think of two main reasons for that. On the one hand, research methods in SSH have predominantly been qualitative where software has played only a supporting role. Such research methods, let's take *discourse analysis*, have always been open, free to use and to modify and redistribute. On the other hand, the quantitative fields of SSH have mostly used statistics or survey and register data, or other, often restricted, research data that proprietary analysis software such as SPSS, Stata or Excel are well suited for. However, the future of SSH looks somewhat different as the quantity and multiplicity of sources of digital data are challeging both the traditional methodologies and popular software tools. The future that Gary @king2014 of Harvard described in 2014 is here already:

>An important driver of the change sweeping the field is the enormous quantities of highly informative data inundating almost every area we study. In the last half-century, the information base of social science research has primarily come from three sources: survey research, end-of-period government statistics, and one-off studies of particular people, places, or events. In the next half-century, these sources will still be used and improved, but the number and diversity of other sources of information are increasing exponentially and are already many orders of magnitude more informative than ever before.

In such data rich SSH research, where the role of software and computation becomes more central, the questions of licensing, ownership, modification and distribution of that software becomes increasingly important. This chapter will introduce you to an open source programming language *R*, one viable option for data analysis and visualization.

<!--

As relevant data for SSH is being generated and published in multiple sites and as the data is often messy and big in size this development calls for flexible tools that can be tailored and modified for the particular data and research question. The custom proprietary tools that are well fitted for analysis of survey, register and statistical data are not capable of adjusting to these emerging individual needs of researcher. Therefore we have witnessed an unseen growth in userbase and developers of free and open source computational research methods. 

And it is not only that there are no proprietary software for these purposes, but the open research methods are also required becaused of
-->

# What is R?

R is a cross-platform software that runs in Windows, Mac OS X and GNU/Linux operating systems. R can be regarded as an implementation of the S language which was developed at Bell Laboratories in 70's by Rick Becker, John Chambers and Allan Wilks  [@venables_introduction_2013]. R supports multiple programming paradigms, and depending on the task it is can be used for instance as an object-oriented or a functional language. Flexibility is one of its strenghts: a same task can be implemented in number of ways, but also its weakness. Especially for someone new to language, it can be a daunting expecience to find the most suitable approach from the many almost alike solutions.

The name R comes from the first names of two statisticians from New Zealand: Ross Ihaka and Robert Gentleman, who created the language in late 1990's for their students. The project was staterted in 1992, with an initial version released in 1995 and a stable beta version in 2000. The latest stable version at the of writing is 3.3.2 published in October 31, 2016. R is distributed under the terms of the *GNU General Public License* as free and open source, and it can be further distributed under those conditions. R is available from [Comprehensive R Archive Network](http://cran.r-project.org/) (CRAN).

The most basic user interface for R is a a command-line interpreter or a  *console*, which allows user to type in commands and outputs the results of the analysis. Most users use R through a various [integrated development environment](http://www.sciviews.org/_rgui/projects/Editors.html) (IDE) that provide the user, in addition to console, number of useful tools from text editors, to file browser and version control integration and code debuggin features. [RStudio](https://www.rstudio.com/products/rstudio/) has gained a lot of popularity in last couple of years and is available for all major desktop platforms and as a linux server version. Microsoft also introduced a support for R in its Visual Studio IDE in late 2016 as a part of their larger strategic move towards R by [acquiring R consultancy compary *Revolution Analytics*](https://blogs.technet.microsoft.com/machinelearning/2015/04/06/microsoft-closes-acquisition-of-revolution-analytics/), joining [R consortium as a platinum member](https://www.r-consortium.org/members) and introducing its own R open version of R and CRAN at [The Microsoft R Portal ](https://mran.microsoft.com/). In addition, there are extensions for using R in many open sourc IDE such as Eclipse or Emacs.

## Extending R with user contributed packages

Official name [The R Project for Statistical Computing](http://www.r-project.org/) refers both to the a) centrally maintained core of the language as well as to b) R's distributed structure of contributed extensions, called *packages*. Basic R installation consists of *base installation* of language core with some 25 additional *packages* for the most basic functionality. The core of the language is maintained by *R Development Core Team*, but the additional packages are developed and maintained by individual developers and research institutes. Packages in R are collections of functions and/or data that are packaged for conviniency. Installing a package broadens your the functionality of your R installation.

CRAN is the official repository for user contributed packages and by the end of January 2017 has over 10 000 packages that can be used to extend R. The R-code below creates the figure 1, by downloading and processing information from a single CRAN mirror on currently available packages and their archived earlier version to create a dataset of package name and date when its first version was submitted to CRAN.

```{r, include=FALSE}
suppressMessages(Sys.setlocale("LC_TIME", "C"))
```


```{r packages, cache=TRUE, fig.height=7,fig.width=9, message=FALSE}
library(XML)
library(tidyverse)
library(extrafont)
loadfonts()
if (!file.exists("./data/packages.RDS")){
current <- readHTMLTable(readLines('https://cran.rstudio.com/src/contrib'),
                                which = 1, stringsAsFactors = FALSE)
names(current)[c(1:3)] <- c("col1","name","date")
current %>% 
  filter(Size != "-", 
         grepl("tar.gz$", name)) %>% 
  mutate(name = sub('^([a-zA-Z0-9\\.]*).*', '\\1', name)) %>% 
  select(name,date) -> pkgs

packages <- data_frame()
for (i in 1:nrow(pkgs)){
  path <- paste0('https://cran.rstudio.com/src/contrib/Archive/',pkgs$name[i])
  if (httr::GET(path)$status == 200){
    tbl <- readHTMLTable(readLines(path), which = 1, stringsAsFactors = FALSE)
    names(tbl)[2:3] <- c("name","date")
    new_row <- tbl[3,c("name","date")]
    new_row$archived <- TRUE
  } else {
  new_row <- data_frame(name = pkgs$name[i], date = pkgs$date[i], archived = FALSE)
  }
 packages <- bind_rows(packages,new_row)
}
saveRDS(packages, file="./data/packages.RDS")
} else packages <- readRDS("./data/packages.RDS")
packages %>% 
  mutate(date = as.POSIXct(date, format = '%d-%b-%Y %H:%M')) %>% 
  arrange(date) %>% 
  mutate(rank = 1:nrow(.)) -> packages

packages %>% filter(rank %in% seq(0, 10000, by = 1000)) -> vdat

p <- ggplot(data=packages, aes(x=date,y=rank))
p <- p + geom_segment(data=vdat, aes(xend=as.POSIXct(-Inf, origin = "1970-01-01"), yend=rank), color="grey80")
p <- p + geom_segment(data=vdat, aes(xend=date, yend=-Inf), color="grey80")
p <- p + geom_point(alpha=.1, shape=1, size=.5, color="sienna1")
p <- p + geom_text(data=vdat, aes(x=date,y=rank,label=format(date, format="%d-%b-%Y")), hjust=1, vjust=-.2, size=2.5, color="dim grey")
p <- p + theme_minimal() + theme(text=element_text(family = "Open Sans")) 
p <- p + theme(panel.grid.major = element_blank(),
               panel.grid.minor = element_blank())
p <- p + scale_y_continuous(breaks=seq(0, 10000, by = 1000))
p + labs(title="Number of packages currently published in CRAN",
              subtitle="Based on script by Gergely Daróczi at https://gist.github.com/daroczig/3cf06d6db4be2bbe3368
Each dot represents a publication date of a package",
              caption="Data: https://cran.rstudio.com/src/contrib/",
              y="number of packages",
              x="date when first submitted to CRAN")
```

In addition to CRAN, various code hosting sites such as [GitHub](https://github.com/) have become increasingly important resources for collaboration and dissemination of new packages. [Bioconductor](http://www.bioconductor.org/) is another separate package repository, but can be regarded as *domain spesific* for it hosts packages for *the analysis and comprehension of high-throughput genomic data*. Another such domain spesific project is [rOpenSci](http://ropensci.org/) that provide R-based tools for open science.


## Learning the language

<!-- There are several graphical user interfaces (GUI) in R that provide a point and click -type of user interface that may be helpful in the beginning, such as [RCommander](http://www.rcommander.com/) or [Deducer](http://www.deducer.org/). However,  -->

As the internet has brought together the vast community around R, the internet has become the main channel for delivering instructions for R. The official *Introduction to R* by @venables_introduction_2013 is an important document to get familiar with when learning the language. In addition R-project is organized into a domain spesific sections where you can start learning from so called [task views](http://cran.r-project.org/web/views/). For SSH researcher the [social sciences](http://cran.r-project.org/web/views/SocialSciences.html), [natural language processing](http://cran.r-project.org/web/views/NaturalLanguageProcessing.html) and perhaps [Web Technologies and Services](https://cran.r-project.org/web/views/WebTechnologies.html)task views are good places to begin with. 

[Official mailing lists](http://www.r-project.org/mail.html) have traditionally hosted the discussions, but over the last couple of years "Question & Answer"" -sites like [Stack Overflow](http://stackoverflow.com/questions/tagged/r) have challeged them especially in delivering solutions for one-off user questions. Stack Overflow has currently 168 025 questions tagged with R. In comparison to proprietary software, there are 7557 questions tagged with SAS, 2246 with Stata and 998 SPSS. Besides the Question & Answer sites, there are hundreds of blogs discussing specific analytical problem using R and feeds from the blogs are aggregated in [R-bloggers](http://www.r-bloggers.com/)-website. 

Another, more formal channel for distributing and communicating R have become the so called massive open online courses (MOOC). MOOCs seem to work well for teaching programming and many courses in [Coursera](https://www.coursera.org/) and [EdX](https://www.edx.org/) have become hugely popular attracting tens of thousands of students each year. The free licensing of R has made it primary language on these coursesas it is basicly the only viable alternative for teaching statistical programming for massive crowds.

Aside with vibrant internet community more and more books are being published on R. Books can be put in three categories. First are the general introductions to statistics using R. *R for Data Science* by @wickham_r_2017 and *R in Action: Data Analysis and Graphics With R* by Robert @kabacoff_r_2013 are popular examples of that category. Second there are more and more books addressing how to solve some specific analytical problems using R. A prime examples of books in this category are *Complex Surveys: A Guide to Analysis Using R*  by Thomas @lumley_complex_2011, *Text Analysis with R for Students of Literature* by Matthew @jockers_text_2014 and *Applied Spatial Data Analysis with R* by @bivand_applied_2013. A third category are the books that focus on specific theoretical issue in statistics and use R as a primary language to demonstrate this. Such books are for instance *Bayesian Data Analysis* by @gelman_bayesian_2013 or *Multilevel Analysis: An Introduction to Basic and Advanced Multilevel Modeling* by @snijders_multilevel_2011.

## Who uses R?

Throughout it's existence the main use of R has been implementation of new statistical methods. This is still the case and implementations of new statistical methods are usually first available in R. However, various fields of applied statistics have become more active as researchers across disciplines have started to migrate into R. *Bioconductor* was already mentioned as an example of a domain spesific initiative to apply R in their analysis, for genome data in this case. Natural sciences in general have been early adopters and for example in Geographical Information Systems (GIS) the R has started rival proprietary GIS-software. As for humanities,  Matthew @jockers_text_2014 book is one of the first attemps to foster use of R. Forthcoming book *Text Mining with R.  A tidy approach* by @silge_text_2017 will be another interesting attemp to extend analysis of text in R. In addition, [rOpenGov](https://github.com/rOpenGov/) is one active community developing algorithms for computation social sciences and digital humanities worth following. Site [the Programming Historian](http://programminghistorian.org/) offers novice-friendly, peer-reviewed tutorials that help humanists learn a wide range of digital tools, techniques, and workflows to facilitate their research.


R is no longer niche software for academics, but has also become a major player in business analytics, too. R was originally designed as an interface language: to provide a consistent language interface for algorithms written in other languages. These days several packages provide language bindings to other open-source software such as C or Java, making R a convenient hub for all kinds of algorithms and methods, both for open source and proprietary. In addition to Microsofts, for instance Oracle has its [Oracle R Enterprise](http://www.oracle.com/technetwork/database/database-technologies/r/r-enterprise/overview/index.html) providing R integration into their commercial products. IBM also recently [joined R Consortium]((https://www.r-consortium.org/announcement/2016/06/06/ibm-invests-in-r-programming-language-for-data-science-joins-r-consortium) and has steadily improved R integration in its products, despite having acquired SPSS in 2009. Also major newspapers like [Financial Times](http://johnburnmurdoch.github.io/slides/r-ggplot/#/), [New York Times](http://datastori.es/ds-56-amanda-cox-nyt/) use R extensively in their data journalism and data visualizations.

# Conclusions

R is certainly not the only alternative for proprietary data analysis software or for analysis of complex digital data. For example, [Python](http://www.python.org/) is another viable option especially for someone looking for more general purpose language that also masters data analysis. The data analysis is becoming mainstream on many fields, not just in academic research, but R is still remaining hard to learn and very much researcn oriented. Programmers rather want to extend the language they already know than learn a new one and python is a lot more common than R. For very intensive computation [Julia](http://julialang.org/) is becoming popular open source option, too. It is still in early phase of development, but is already viable option when performance is crucial.

The data intensive research in SSH is here to stay and for someone wishing to become successful in it, it is equally important to develop skills and understanding in these emerging technologies as it is to develop substantial understanding of the research topic itself. R is a prime example of this development where academics have taken a major role in software development and created tools for their own needs that are overriding proprietary software also in business intelligence.

And what stand out here is the *licensing* of the software. Free and open source software such as R allows you to study how the code works, write improvements and share them with others. In addition, free software licensing also allows you to teach the technology, apply it in any purpose, including commercial, and to apply your skills in any organisation, despite their choices of software. Open source research software is not always the easiest way to get job done, but in the long run it is worth the time invested. 

Openness of your software is crucial also from the *reproducibility* of your research. Alongside with demands for open access of research publications, more and more journals are requiring both the data and algorithms behind the results to published together with the research. As SSH are adopting computational analysis the issue of reproducibility should also be kept in mind. 

Steve @lohr_literary_2013 interviewed some leading digital humanists in New York Times article *Literary History, Seen Through Big Data’s Lens* on the future of SSH and posed a question whether these emerging computational technologies will undermine the role qualitative research in the field. Matthew Jocker, whose book *Macroanalysis: Digital Methods and Literary History* [-@jockers2013macroanalysis] was central in the article, emphasized that finding the right questions and flaws in the analysis still requires deep, both qualitative and quantitative, understanding of the field:

>But we’re at a moment now when there is much greater acceptance of these methods than in the past. There will come a time when this kind of analysis is just part of the tool kit in the humanities, as in every other discipline.

And that:

>Quantitative tools in the humanities and the social sciences, as in other fields, are most powerful when they are controlled by an intelligent human. Experts with deep knowledge of a subject are needed to ask the right questions and to recognize the shortcomings of statistical models.

The quest for new kind of collaboration between scholars and fields of research is also emphasized by professor Gary @king2014. He claims that the analysis of large digital data requires skills that can't be found from traditional fields of social sciences.

>Through collaboration across fields, however, we can begin to address the interdisciplinary substantive knowledge needed, along with the engineering, computational, ethical, and informatics challenges before us.

In addition, @king2014 assumes that this collaboration will eventually blur the dichotomy between qualitative and quantitative analysis, and he portraits a future where both traditions have merged into social sciences where the important research problems are solved in collaboration.

> Instead of quantitative researchers trying to build fully automated methods and qualitative researchers trying to make due with traditional human-only methods, both now are heading toward, using, or developing computer-assisted methods that empower both groups. This development has the potential to end the divide, to get us working together to solve common problems, and to greatly strengthen the research output of social science as a whole.

 
# References



